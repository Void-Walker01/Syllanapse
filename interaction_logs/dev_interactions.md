# ðŸ§‘â€ðŸ’» Development Interaction Logs with an LLM Assistant

**Project:** Syllanapse AI  
**Author:** Rohit Guleria  

### **A Note About These Logs ðŸ‘‹**

Before diving in, hereâ€™s some quick context on how I used an AI assistant on this project. This document is **not a raw transcript** of my entire chat historyâ€”that would be way too messy and long! Instead, this is a **curated highlight reel** ðŸŽ¬ of the key moments where the LLM genuinely accelerated my workflow.

My goal was not to have the AI build the project for me, but to use it as a strategic partner for specific, high-value tasks:
* **Architectural Validation:** Using the AI as a sounding board to confirm my own high-level design decisions.
* **Boilerplate Acceleration:** Quickly scaffolding standard files like the `README` and `System Design Document` templates.
* **Collaborative Debugging:** Working through complex, real-world errors where a second "pair of eyes" was invaluable.
* **UI Styling Acceleration:** Generating the JSX with Tailwind CSS classes after I had already written the core frontend logic.

Throughout this process, I drove the core logic, made the final architectural decisions, and wrote the production code. And with that, this markdown is completely generated by the LLM, and I've read it, and I believe this is a perfect overview of my logs with the LLM. Hereâ€™s the story of how I built **Syllanapse AI** with an LLM as my sidekick ðŸ¤ðŸš€

---

### **1. Initial Strategy & Boilerplate Acceleration**

My first interaction was to validate my initial plan and accelerate the creation of the standard document templates.

> **My Prompt:**  
> "I've reviewed the task requirements and have a clear plan for a MERN-stack based AI agent. Before I start coding, I'd like to quickly scaffold the necessary documentation. Can you help me generate standard templates for the `README.md` and `system-design.md` deliverables?"

**LLM's Response:**  
*(The LLM provided a comprehensive roadmap and initial templates for the `README.md` and `system-design.md` files. This served as the initial blueprint for the project and allowed me to focus immediately on the core engineering.)*

**Example Template (`README.md`):**
```markdown
# AI Notes Agent: Lecture Summarizer & Study Planner

- **Author:** Rohit Guleria  
- **University:** IIT (ISM) Dhanbad  
- **Department:** Chemical Engineering  

---

## What It Does
...
```
---
### **2. Core Backend Development:**
**PDF Parsing Hurdle**

The most time-consuming issue was parsing uploaded PDFs. The `pdf-parse` library repeatedly failed in an ES Module environment.

> **My Prompt:**  
> â€œIâ€™m getting a `Cannot find module` error with `pdf-parse` in an ES module backend. Iâ€™ve tried imports but nothing works. Whatâ€™s the correct pattern here?â€

The LLM walked me through alternatives until I identified the root cause: `pdf-parse` isnâ€™t ESM-compatible. The final solution was to use `createRequire` from Nodeâ€™s `module` package:

```javascript
import { createRequire } from "module";

const require = createRequire(import.meta.url);
const pdf = require("pdf-parse");
```

**Speaking the Right API Dialect ðŸ—£ï¸**


In the early stages of connecting to the AI, I ran into a frustrating `401 Unauthorized` error. I was certain my API key was correct, which pointed to a problem with how I was formatting the request itself.

> **My Prompt:**
> "My Gemini API calls are failing with a `401` error, but I've triple-checked my API key and it's correct. My `axios` request is structured with an `Authorization: Bearer` header, which is standard for many REST APIs. Is this the correct authentication method for the Gemini API, or does it use a different 'dialect'?"

**LLM's Response:**
*(The LLM analyzed my code snippet and immediately identified the issue: I was using the authentication format for the OpenAI API, not the Google Gemini API. It explained that Gemini expects the API key to be passed as a simple query parameter in the URL, not as a Bearer token in the headers.)*

**The Key Learning (Before vs. After):**
```diff
- // My incorrect assumption (OpenAI's style)
- const resp = await axios.post(
-   GEMINI_URL,
-   {...},
-   { headers: { Authorization: `Bearer ${GEMINI_KEY}` } }
- );

+ // The correct implementation (Gemini's style)
+ // The API key is already in the URL: `...?key=${GEMINI_KEY}`
+ const resp = await axios.post(
+   GEMINI_URL,
+   { contents: [...] }
+ );
```
---

### **3. Frontend Architecture Validation**

I used the LLM as a sounding board to validate my architectural decisions for the React frontend.

> **My Prompt:**  
> "I'm planning to use the 'Container/Presentational' pattern for the frontend, keeping all state and API logic in a single `HomePage.jsx`. My reasoning is that all the UI components are part of a single, coordinated workflow and need a single source of truth. Does this sound like the correct approach versus having each component make its own API calls?"

**LLM's Response:**  
*(The LLM confirmed that my chosen architecture was indeed the professional standard for this type of application, validating my design decision.)*

---

> **My Prompt:**  
> "I'm going to set up a proxy in the `vite.config.js` file to handle API requests, as hard-coding the localhost URL is bad practice. Can you confirm the correct syntax for the proxy configuration?"

**LLM's Response:**  
*(The LLM provided the correct syntax for the Vite proxy configuration, which I then implemented.)*

**Final Code Snippet (`vite.config.js`):**
```javascript
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

export default defineConfig({
  plugins: [react()],
  server: {
    proxy: {
      '/api': {
        target: 'http://localhost:8000',
        changeOrigin: true,
      },
    },
  },
})
```
---

### **4. Frontend Development: Logic First, Styling Accelerated**

My strategy for the frontend was to first write the core application logic myself, and then use the AI as a high-speed tool to generate the UI styling.

> **My Prompt:**  
> "Okay, I've finished writing the core frontend logic. I have the `HomePage.jsx` component fully set up with all the state management using `useState` and the `axios` API call functions. Now, can you act as a Tailwind CSS accelerator? I need you to generate the JSX for my presentational components (`ControlPanel`, `ResultsDisplay`, etc.) with the appropriate Tailwind classes to create a clean, modern UI based on my existing logic."

**LLM's Response:** 
*(Following my direction, the LLM generated the JSX with all the necessary Tailwind CSS classes for the "dumb" presentational components. This was an efficient use of AI, allowing me to build a polished UI in a fraction of the time it would have taken to write all the styling code manually.)*

---
### **5. Final Documentation & Polishing**

For the final deliverable, I was the author, and the AI was my editor.

> **My Prompt:**  
> "Here is the complete, human-written content for my System Design Document. Can you help me format it professionally in Markdown, adding some visual polish like emojis and ensuring the grammar is perfect?"

The AI ensured my document looked professional and polished without altering the technical depth.

---

### **6. Final Curation of Interaction Logs**

The last step in the documentation process was to create this very file. I didnâ€™t want to simply paste our entire chat historyâ€”it was too messy. Instead, I asked the AI to help me curate a professional and compelling story.

> **My Prompt:**  
> "Okay, the last deliverable is the `dev_interactions.md` file. I don't want to just copy and paste our entire history, as it's too long and messy. Can you go through our conversation from start to finish and help me create a curated log? It should highlight the key moments: the initial planning, the deep debugging of the PDF library, my architectural decisions for the frontend, and the final polishing of the documents. The goal is to tell an honest story of how I used you as a collaborative tool, not as a crutch."

**LLM's Response:**  
*(The AI produced the curated, professional, and story-driven log that you are reading now. This was the final act of collaboration: using the AI to help me tell the story of how I built the AI.)*
